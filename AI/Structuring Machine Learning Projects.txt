ml strategy

정확도가 충분치 못할떄
데이터 수집하거나,더 오래 학습시키거나,최적화알고리즘 적용하거나등 여러 방법이 있다.

이런 개선방법이 많아서 뭘 선택하는지가 중요.

직교화:각 조정의 결과가 서로에게 영향을 끼치지 않게하는것.(각 조정하는 벡터가 서로에게 직교하는것.)(다른 차원)

training set costfunction기준 성능 통과(더 큰 네트워크,adam등)
->devset costfunction기준성능통과(정규화,bigger training set등)
->test set기준 costfunction성능통과()
->real world적용의 구조(devset교체나 costfunction교체)

단일 실수평가지표를 사용할것을 권장함
F1score= ave(P and R)
(precesion and recall)
precesion은 인식한 대상중 얼마가 진짜인지
recall은 전체중 얼마가 정확히 인식되었는지

만족지표,최적화지표
만족지표는 어느정도 이상이면 충분한 지표,
최적화지표는 목표값에 가까울수록 좋은 지표

trainig/dev/test distrubution
분포도가 비슷해야함.

개발셋과 목표지표를 결정할때 분포도 자체가 다르면 문제생김.

같은 분포에서 dev,testset이 만들어져야함.

dev,test set 길이.
초기엔 70,30으로(training,test)
혹은 60,20,20으로(training,dev,test)로

근래에는 100만개정도를 다루면 98%traning,1%,1%로 나눔

test set은 충분히 성능평가가 될만큼 크게. 성능평가가 필요없으면 없어도됨.

Metrix(분류기)재설정등이 필요할때:
성능지표상으로는 나은데 실제론 다른 지표가 고려되어야함을 알았을때, 분류기를 재설정하거나
(가중치 지표를 넣는듯(새로운 판단기준의 오류율에서 가중치를 높게줌)) devset이나 traningset을 재설정해야함.
가중치 도입을 위해선 새로운 traning,dev거쳐야함.
원하는 결과 안나오면 이처럼 새로운 평가 지표 넣어야함.

첫째로는 평가지표를 어떻게 구성할지 생각하고(각 지표들을 직교화를 고려해 구해야)
(좀더 정확한건 예전의 머신러닝 수업들은거 복습해라)
그 다음에 실제로 이걸 어떻게 구현할지 생각해라.(즉 이전단계부터 고려하지는 마라)

인간의 오류수준과 비교(대개 베이 오류율에 근접)해서 줄일수 있는 부분을 정한다(분산,편향중)

인간수준 성능이라는건 bayes error를 추정하는 방법이다(최선의 오류값)(적어도 그것보단 낮으니까)

자연 인지되는것(인간에게)은 인간성능을 뛰어넘기 어렵다. 
구조화된, 방대한 데이터를 다루어야하는 경우에 인간성능이상을 뛰어넘는걸 목표로 해볼 수 있다.
물론 음성,이미지인식에서도 그런경우(인간성능이상)가 있으나 구조화된 데이터 다루는 쪽이 가능성있음.

정리하면
training set costfunction기준 성능 통과(더 큰 네트워크,adam등)
->devset costfunction기준성능통과(정규화,bigger training set등)
->test set기준 costfunction성능통과()
->real world적용의 구조(devset교체나 costfunction교체)
여기서
1번째 단계에선 bias를
2번째 단계에선 variance를 줄이는 방식으로 직교화하고

human level과 비교해 traningerror의 줄일수 있는 편향을,dev error와 비교해 줄일수 있는 variance를 구하고
bias를 줄이는건 최적화 알고리즘, 더 큰 모델,NN 구조나 하이퍼 파라미터등 사용.

variance를 줄이는건
더 많은 데이터 얻거나, 정규화(L2,dropout등)
NN구자나 하이퍼파라미터등 사용.


오류 분석
100여개까지 잘못 분류된 예제를 보고
그중 몇개가 특정 카테고리에 속하는지 본다
특정 비율 이상이면 그 분류 자체를 해결하는데 중점을 두고. 이하이면그렇게까지 할필요는 없는것.

어떤 카테고리에 속하는지에 대해 행렬처럼 두고 잘못분류된 예제들을 분류해서 어느쪽의 문제인지 분류해 통찰을 얻을수 있다.

Y값자체가 사실 잘못된경우:
딥러닝 알고리즘이 랜덤한 오류에 강하기 때문에 일부 오류들이 무작위하게 나타나면 무시가 가능하다.(너무 크지않으면)
시스템적 에러인경우는 문제가됨. 무작위하게 나타나지 않아서 하나의 특징으로 분류가 가능해지면 문제가 된다는 의미.(흰 강아지를 고양이로계속분류하는 예)

위의 오류분석 행렬에 잘못된 라벨링 항목을 넣어서 평가한다

전체 dev set error를 보고, devset에서 잘못된 y값으로 인한 오류가 얼마인지 보고,나머지 이유로 인한 에러들을 본다. 전체비중(devset에서)오류의 원인으로 차지하는 비중이 작으면 다른게 우선.

devset의 목적은 분류기들 사이의 선택을 하기 위한 것이다.

dev/testset correcting guidrain
어떤 방식의 프로세스를 적용하던 개발,시험셋에 동시에 적용할것.

알고리즘이 맞췄던것,틀렸던것을 둘다 볼것.

훈련셋에서의 수정의 필요성은 적다.

학습프로그램 만들때는 시스템을 빠르게 만들고(개발,시험셋과 지표를 정하고(목표를 빠르게 정하고)) 초기 머신러닝 시스템을 만들고 반복하라. 
이후 편향,분산분석,오류분석을 해라. 이후 성능향상을 위한 우선순위를 정하다.

이미 논문등이 많은경우에는 그걸 참고하는게 좋지만, 아닌경우에는 먼저 만들고 생각하는게(장고끝에 지저분한 설계를가지는게) 더 안좋은 방법이다.

훈련셋과 테스트셋의 분산이 다를경우

데이터의 분산이 상이할경우(목표분류대상이 적고 다른 데이터만 많을경우):둘다 섞는게 하나의 방법. 장점은 훈련,개발,시험셋이 모두 같은분포에서 오는것
단점은 중요하게 생각하는 데이터셋(분류목표)가 아니라 다른게 대부분일수 있다는것.

두번째 방법은 ,dev,test set을 분류목표데이터로 구성하고 훈련셋은 그 나머지들과의 합으로 구성하는것.

Bias and Variance with mismatched data distrubition:
두번째 방법인경우 dev error가 더 큰게 정상인데 얼마만큼의 오류가 둘이 달라서 생기는 오류인지 알기 어려움.

이를 위해 training-devset을 정의, traningset과 같은 분포를 가지는 devset을 정의해 이 devset을 가지고 traning error와 비교해 동분포에서 얼마만큼의 오류가 차이나는지 보는것.

traning,training-dev가 비슷하고 deverror만 크면 데이터 불일치 문제고 이경우에는 알고리즘 자체가 이전분포에서만 적응한것이다.

또 목표데이터에 대한 trainigset에서 error,human error를 구해서 이것과의 dev error를 비교해 목표데이터셋에서의 회피가능한 편향,분산문제를 본다. 

데이터 불일치 문제다루기
1.수동 오류분석수행해서 특징들을 잡는거
2.traning data를 비슷하게 하거나(특징을 가지도록 데이터를 합성) 데이터를 더 수집한다.

2주차 learning from multiple tasks부터