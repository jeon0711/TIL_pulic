2주차부터
Optimization Algorithms:
여러 모델들을 쓰고 그중 잘되는거 찾아야함.
그러므로 최적화로 빠르게 해보는게 중요
mini-batch gradient descendt
전체훈련세트를 더 나눠서 1000개정도의 셋을 하나의 훈련셋이 가지게 되도록로 나눔
그리고 1000개를 동시에 순전파(벡터화해서)
코스트함수는 합을 1/1000해서 구한다.
역전파도 동일하게 

mini batch에서는 비용함수가 증가할수도 있다.
모든 반복마다 감소하지 않아도된다. 전체적으로 감소하면 되므로

미니배치의 크기를 골라줘야한다.
미니배치의 크기는1~m까지,m이면 전체이고 1이면 확률적 경사 하강법을 쓴다.

확률적경사하강법은 절대 수렴하지 않고 진동,m은 너무 크고 1은 너무작다. m은 시간이 너무 길게걸린다. 확률적경사하강법은 훈련샘플을 하나씩 처리하기에 너무 비효율적. 너무 크거나 너무 작지 않아야 효율적.

적당해야 전체다 끝나기 전에 진전을 시킬수 있다. 묶은 훈련세트 전체가 끝나야 결과가 나오므로.

작은 훈련셋인경우 batch(전체)트레이닝하는게 맞음. 2000보다 작을때,
이거보다 크면보통 64~512로 mini batch 컴퓨터 메모리 접속방식에 따라 2의 거듭제곱일때 대개 빠르기에 이렇게 하는거,1000으로하려면 1024로할것 추천

미니배치가 cpu,gpu 내부 메모리에 들어가는걸 보장하는게 낫다.(크기를 조정해서)

exponentially weighted (moving) averages
(지수가중평균,지수가중이동평균)
지수평활같은데?맞다. b*Vn-1+(1-b)*a=Vn
시계열 데이터등

지수평활 bias correction
초기값을 0으로 하고 시작하는데 
기존에 구한값=Vt/1-B^t->vt로 하는것
t가 커지면 0으로 수렴하므로

Gradient descent with momentum
모멘텀,혹은 기울기 모멘텀. 
매번 반복 t마다 dw,db를 산출할때
Vdw=bVdw+(1-b)dw로 계산하여 Vdc=bVdc+(1-b)dc로 연산.
(V항은 새로 연산하는 속도항이다,Vdc,Vdw=0로 초기화되 시작)
W=W-aVdw로 update,c=c-aVdb로 update한다

bVdw+dw=Vdw등으로 나타내는건 1-b를 생략한 표현이고 실제로는 들어가야한다.

RMSprop부터


RMSprop(rootmean square prob)
매 반복에 dW,dB를 계산하고 
Sdw=a*Sdw+(1-a)dw^2(요소 전체의 제곱)
Sdb=a*Sdb+(1-a)*dB^2을 계산해
W=W-q*dw/Sdw^1/2로 업데이트하며 b역시도 동일하게 update

진동을 폭을 줄이는게 목적,따라서 더큰 학습률을 사용가능하게됨

sdw가 0에 가깝지 않다는거 구현하기위해 Sdw^1/2+엡실론으로 나눠야한다.10^-8정도

Adam optimization algorithm
Vdw=0,Sdw=0,Vdb=0,Sdb=0
로 초기화, compute dW,db를 매 반복마다 minibatch에서 구하고 Vdw=aVdw+(1-a)dw로 Vdb도 동일하게 구함
Sdw=cSdw+(1-c)dW^2(각 요소들의 제곱, dW^2[i][j]=(dW[i][j])^2)으로, Sdb도 동일하게 구함
Vdw=Vdw/(1-a^t)
Sdw=Sdw(1-c^t),db에 대해서도 ㄱ하고 최종적으로 W=W-q*Vdw/Sdw^1/2+e
로,b도 동일하게 update

learning rate decay(학습률감소)
처음엔 크게,나중엔 작게하기위해
(위의 W-q*Vdw..에서의 q이다.)
1.q=q0/1+decayrate*epochnum으로 설정하는것(decayrate는 설정하는거고 epochnum은 학습 데이터 넘버링값)
예제에선 decay-rate를 1로,q0=0.2 하고있음

부분최적값 문제(부분최적해에 갇히는 문제)
매개변수 수가 클수록 local optima에 갇힐 확률은 낮다. 대신 안장점인 경우가 많을것. 안장점인 경우 obsilation에 의해 벗어나게 된다. 그래도 안장점에서 학습이 느려지기에 adam등이 쓰이는것.

learning rate(제일 중요),momentum(0.9default)등 e등 여러 hyperperameter세팅이 어려움(mini batch size까지)

grid쓰지 말고 random value쓰자. 임의로 정하면 같은수의 hyperparameter놔두고 다른거 바꾸면서 하게되는데 이거보단 같은횟수를 돌려도 random하게 바꾸는게 나음 그럼 몇배 많은 경우를 시험하게 되므로.

랜덤하게 하고, 잘되는 것들이 모여있는 부분의 random hyperparmeter를 다시 시험한다. 그범위 내에서 다시시행.

uniformly random하게 하는 거보단 log scale로 하거나(python 기준 r=-4*np.random.rand()
a=10^-r)꼴로 로그스케일 가능.

응용해 지수가중평균(exponentailly weighted averages)의경우
b=0.9~0.999,이므로 
b=10^r
r=[-3~-1]로 하면됨.

충분한 자원이 있으면 병렬로 돌리고 아니면 하나 집중해서 하라

batch normalization부터

배치 정규화
input을 input의 표준편차로 나눠서 정규화

hidden layer의 정규화,활성화 함수 이전의 값을 정규화(z값)
z값의 평균,표준편차 구하고 z값의 평균을 뺴고 표준편차로 나눠서 정규화한다.
정규화 후의z는 평균0,분산1이됨

은닉유닛이 다들분포를 가지게 하려 하면
z[i]=a*z[i](norm)+b,a,b는 learnable parameter로 이것도 학습대상이 될수도 있음.

deep network에 적용하면
X-W,B->Z-a,b->Z(norm)->A=g(Z)

이제 파라미터는 W,B,a,b들이됨.
db를 구해서 b-학습률*db로 업데이트

mini batch에서 X[1]으로 Z를 구해 정규화하는 동안 어차피 B[l]은 날아가니까 batch norm을 적용하는경우에는 이걸 날려버릴수 있다.

가중시키는 부가효과도 있어서 더 잘 작동한다.

입력의 공분산이 변화하는 경우에 다른 결과를 낼수가 있는데.
은닉단위 값들의 분포의 평균과 분포가 변하는 것을 막아준다.

미니배치로 정규화하면 부정확하므로 생기는 무작위적인 노이즈가 무작위 변동 역활을 함으로서 더 다양한 패턴 학습,일반화가능

배치정규화는 배치하나를 처리하지만 테스트시에는 그렇지 않으므로 좀 다르게 할 필요가 있음. 
지수가중평균으로 미니배치들에서의 평균과 표준편차로 평균과 표준편차를 추정해간다. 테스트시에는 학습완료시의 이 지수가중평균으로 구한 평균과 표준편차로 계산한다.

softmax regression
로지스틱 회귀의 일반화
logistic regression에서 이진분류가 아니라 다수분류
C=number of class
마지막 판정을 하는 layer의 노드수는 일반적으로 C와 같다 각각이 해당 클래스인지 판정하는거

softmax activation function
e^z[l]/sumof(e^z[l])
최대 우도 추정법맞다

이하는 복습

logistic regression
선형 모델로 만들고, p(X)=e^f/1+e^f.
odd=pX/1-pX=e^f,p일 확률/아닐확률=e^f가 되도록 f를 정한다.
p를 알때 e^f를 결정할수 있는것.
px를 안다면 log(px/1-px)=f(x)가 되도록 f정한다.
선형비례관계는 아니지만, 비례관계가 존재하면 이를 표현할수 있다.
maximize likelihood function(a,b)=(yi=1이면 px,아니면 1-px의 곱들.)이다. 곱하는거맞다. 특별한 가정이 들어가면 rss되는것.
최대화한 거 구하는 수학적 디테일은 여기서는 넘어감.
a,b는 f가 선형일때 a는 계수벡터,b는 상수값이다.
최대화하는 a,b를 구했다면, a,b가 모수일떄 a,b는 무선적으로 추출한 데이터셋에서 결정되는,최대화하는 a,b값이 되며, 이값들을 무선적으로 표본추출해서 나오는 표본평균들로 만든 분포로 검정을 하는거다.

likelihood(우도,가능도)함수는 입력이 모집단을 생성할때, 이 모집단일때 현재 표본이 나올 확률이다. 현재 표본이 나올 확률이 높은 모집단을 고르는것으로 추정하기 위해 사용되며, 총합이 1일지는 모르겠다 즉 모집단의 확률은 아님.베이의 추정법으로 likelihood에서 모집단의 확률 구할수 있는 경우도 있다.

z분포구해서 H0:w=0 인지 본다.
p-값은 영가설을 잘못기각하는 결론으로 이끌 확률이다.
 
단순 다수인자인경우 위의 방법을 쓰면 되지만, 결과값 y가 여럿인경우
pr(yk|X)=sum(e^f(w(k),x))/1+sum(k=1~N-1,sum(e^f(w(k),x)))
log(pr(yk|x)/pr(y=K|x))=f(w(k),x)이다.
y=n일때는 분자가 1이며 전부 아닌경우를 의미한다.
각 y들은 배반사건이라는것 명심. y=n에서 n을 기준으로 할 사건으로 정해도 된다.
이경우 p(y=k|x=x)=e^f(k)/sum(k=1~N,sum(e^f(w(k),x)))
그냥 N까지 더하면됨.
이러면 log(Pr(y=k|X=x)/Pr(y=m)|X=x)=f(w(k)-w(m),x)가 되므로 편함.

복습끝
각a[l]=e^z[l]/sumof(e^z[l])
이 값들중 가장 큰값을 선택한다.

hardmax는 값이 이진수로 떨어져서 인듯

how train
L(y^,y)=-sumof(yjlogy^j)로 계산한다. y^은 1이하의 수이고 yj는 1아니면 0이므로 -log(y^j)가 되고 lossfunction의 최소화는 y^j의 최대화(yhat을 y^로 표기,예측값의미)
J=sumof(L(yj^,yj))/m이된다.

backprop의 dz[l]=yhat-y가 된다(dJ/dz=dz[l]표기)
그럴려고 lossfunction을 저렇게 정한거임.
