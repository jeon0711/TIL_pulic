2주차부터
Optimization Algorithms:
여러 모델들을 쓰고 그중 잘되는거 찾아야함.
그러므로 최적화로 빠르게 해보는게 중요
mini-batch gradient descendt
전체훈련세트를 더 나눠서 1000개정도의 셋을 하나의 훈련셋이 가지게 되도록로 나눔
그리고 1000개를 동시에 순전파(벡터화해서)
코스트함수는 합을 1/1000해서 구한다.
역전파도 동일하게 

mini batch에서는 비용함수가 증가할수도 있다.
모든 반복마다 감소하지 않아도된다. 전체적으로 감소하면 되므로

미니배치의 크기를 골라줘야한다.
미니배치의 크기는1~m까지,m이면 전체이고 1이면 확률적 경사 하강법을 쓴다.

확률적경사하강법은 절대 수렴하지 않고 진동,m은 너무 크고 1은 너무작다. m은 시간이 너무 길게걸린다. 확률적경사하강법은 훈련샘플을 하나씩 처리하기에 너무 비효율적. 너무 크거나 너무 작지 않아야 효율적.

적당해야 전체다 끝나기 전에 진전을 시킬수 있다. 묶은 훈련세트 전체가 끝나야 결과가 나오므로.

작은 훈련셋인경우 batch(전체)트레이닝하는게 맞음. 2000보다 작을때,
이거보다 크면보통 64~512로 mini batch 컴퓨터 메모리 접속방식에 따라 2의 거듭제곱일때 대개 빠르기에 이렇게 하는거,1000으로하려면 1024로할것 추천

미니배치가 cpu,gpu 내부 메모리에 들어가는걸 보장하는게 낫다.(크기를 조정해서)

exponentially weighted averages부터