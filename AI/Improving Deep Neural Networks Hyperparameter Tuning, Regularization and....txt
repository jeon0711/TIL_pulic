2주차부터
Optimization Algorithms:
여러 모델들을 쓰고 그중 잘되는거 찾아야함.
그러므로 최적화로 빠르게 해보는게 중요
mini-batch gradient descendt
전체훈련세트를 더 나눠서 1000개정도의 셋을 하나의 훈련셋이 가지게 되도록로 나눔
그리고 1000개를 동시에 순전파(벡터화해서)
코스트함수는 합을 1/1000해서 구한다.
역전파도 동일하게 

mini batch에서는 비용함수가 증가할수도 있다.
모든 반복마다 감소하지 않아도된다. 전체적으로 감소하면 되므로

미니배치의 크기를 골라줘야한다.
미니배치의 크기는1~m까지,m이면 전체이고 1이면 확률적 경사 하강법을 쓴다.

확률적경사하강법은 절대 수렴하지 않고 진동,m은 너무 크고 1은 너무작다. m은 시간이 너무 길게걸린다. 확률적경사하강법은 훈련샘플을 하나씩 처리하기에 너무 비효율적. 너무 크거나 너무 작지 않아야 효율적.

적당해야 전체다 끝나기 전에 진전을 시킬수 있다. 묶은 훈련세트 전체가 끝나야 결과가 나오므로.

작은 훈련셋인경우 batch(전체)트레이닝하는게 맞음. 2000보다 작을때,
이거보다 크면보통 64~512로 mini batch 컴퓨터 메모리 접속방식에 따라 2의 거듭제곱일때 대개 빠르기에 이렇게 하는거,1000으로하려면 1024로할것 추천

미니배치가 cpu,gpu 내부 메모리에 들어가는걸 보장하는게 낫다.(크기를 조정해서)

exponentially weighted (moving) averages
(지수가중평균,지수가중이동평균)
지수평활같은데?맞다. b*Vn-1+(1-b)*a=Vn
시계열 데이터등

지수평활 bias correction
초기값을 0으로 하고 시작하는데 
기존에 구한값=Vt/1-B^t->vt로 하는것
t가 커지면 0으로 수렴하므로

Gradient descent with momentum
모멘텀,혹은 기울기 모멘텀. 
매번 반복 t마다 dw,db를 산출할때
Vdw=bVdw+(1-b)dw로 계산하여 Vdc=bVdc+(1-b)dc로 연산.
(V항은 새로 연산하는 속도항이다,Vdc,Vdw=0로 초기화되 시작)
W=W-aVdw로 update,c=c-aVdb로 update한다

bVdw+dw=Vdw등으로 나타내는건 1-b를 생략한 표현이고 실제로는 들어가야한다.

RMSprop부터


RMSprop(rootmean square prob)
매 반복에 dW,dB를 계산하고 
Sdw=a*Sdw+(1-a)dw^2(요소 전체의 제곱)
Sdb=a*Sdb+(1-a)*dB^2을 계산해
W=W-q*dw/Sdw^1/2로 업데이트하며 b역시도 동일하게 update

진동을 폭을 줄이는게 목적,따라서 더큰 학습률을 사용가능하게됨

sdw가 0에 가깝지 않다는거 구현하기위해 Sdw^1/2+엡실론으로 나눠야한다.10^-8정도

Adam optimization algorithm
Vdw=0,Sdw=0,Vdb=0,Sdb=0
로 초기화, compute dW,db를 매 반복마다 minibatch에서 구하고 Vdw=aVdw+(1-a)dw로 Vdb도 동일하게 구함
Sdw=cSdw+(1-c)dW^2(각 요소들의 제곱, dW^2[i][j]=(dW[i][j])^2)으로, Sdb도 동일하게 구함
Vdw=Vdw/(1-a^t)
Sdw=Sdw(1-c^t),db에 대해서도 ㄱ하고 최종적으로 W=W-q*Vdw/Sdw^1/2+e
로,b도 동일하게 update

learning rate decay(학습률감소)
처음엔 크게,나중엔 작게하기위해
(위의 W-q*Vdw..에서의 q이다.)
1.q=q0/1+decayrate*epochnum으로 설정하는것(decayrate는 설정하는거고 epochnum은 학습 데이터 넘버링값)
예제에선 decay-rate를 1로,q0=0.2 하고있음

부분최적값 문제(부분최적해에 갇히는 문제)
매개변수 수가 클수록 local optima에 갇힐 확률은 낮다. 대신 안장점인 경우가 많을것. 안장점인 경우 obsilation에 의해 벗어나게 된다. 그래도 안장점에서 학습이 느려지기에 adam등이 쓰이는것.

3주차 hyperparameter tuning부터

