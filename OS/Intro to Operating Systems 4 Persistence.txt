The CPU is connected to the system’s main memory through a memory bus or connection.
A bus is a system for transferring data between components inside a computer or between computers:
Some devices, like graphics and high performance I/O devices are connected to the system using a general I/O bus like a PCI.
Peripheral buses like SCSI, SATA, and USB are used to connect slower devices like disks, mice, and keyboards.

Instead of constantly polling the device, the OS can use an interrupt to:
Send a request
Put the calling process to sleep, and
Switch the context to another job.

When the I/O request is finished, it sends a hardware interrupt and makes the CPU go into the OS to a specific interrupt handler. This handler is OS code that will complete the request, wake up the process that’s waiting for I/O to finish, and lets it continue about its merry way.

Direct Memory Access (DMA) is our answer to this problem. A DMA engine is a system component that handles transfers between devices and main memory without relying so much on the CPU.

The OS tells the DMA engine where and how much data to copy and what device to send data to.
Now, the OS is finished with the transfer and can go on to other tasks.
The DMA engine raises an interrupt when it’s finished, letting the OS know that the transfer is done.

Device communication is generally done in two ways:
Having explicit I/O instructions that define how the OS delivers data to specific device registers.(직접명령)

Memory-mapped I/O is the second method of interacting with devices.

Explicit I/O - Contains instructions that describe how the OS delivers data to specific device registers.

Memory-mapped I/O - Device registers are accessible in the same way as if they were memory addresses.

Device Driver

Device interface functionalities are wrapped within this piece of software called a device driver.

a file system (or application) doesn’t care about disk class. It just sends read and write requests to the generic block layer, which routes them to the appropriate driver.

Certain programs can read and write blocks directly without needing the file abstraction. Most systems support low-level storage management applications using this interface.

harddisk drives부터
-byte blocks) making up the drive. Each one can be read or written to. It’s like an array of n
 sectors, with an address space ranging from 0
 to n−1.

Now that we have a model of the disk, we can analyze its performance. We can represent Disk Access Time as:
TI/O=Tseek+Trotation+Ttransfer
 
Disk Response Time is the average time a request spends waiting for an I/O operation. The average response time is the average response time across all requests.
The rate of I/O (  RI/O
 ), which is often used to compare drives, is computed from the formula below:
RI/O=SizeTransferTI/O

Storage virtualization is based on two main ideas: the file and the directory.
A file is a group of bytes that can be read or written. Each file has a low-level name in the form of an inode number, that users don’t see very often. The file system makes sure that the original file data is always kept on the disk.
In a directory, the user-readable name and the low-level name are listed in pairs.

A program can create a new file by using open() and passing it the O_CREAT flag. The code to the left creates aFile in the current working directory.
The open() call accepts several flags. here:
O_CREAT - creates the file
O_WRONLY - locks it,
O_TRUNC - truncates it to zero bytes, erasing any existing content
S_IRUSR|S_IWUSR - permissions are set to allow the owner to read and write to the file.

open() returns a file descriptor

The open file table shows us the structures of these files as well as the system’s currently open files.

The private descriptor array of each process is linked to a shared open file table entry that connects to the underlying file-system inode. The reference count for file table entries shared by two processes increases until both processes exit the file.
(각각의 프로세스에 있는 filedescriptor는 동일한 파일 참조시 동일한 openfile table을 참조한다)

We expect a file system to keep certain information, called metadata, about each file it stores. To see a file’s metadata, you can use the stat() or fstat() system functions.

A file’s metadata includes information like:
The file’s size in bytes,
Inode number,
Ownership information
When the file was viewed or modified.

struct stat {
dev_t st_dev;         // ID of device containing file
ino_t st_ino;         // inode number
mode_t st_mode;       // protection
nlink_t st_nlink;     // number of hard links
uid_t st_uid;         // user ID of owner
gid_t st_gid;         // group ID of owner
dev_t st_rdev;        // device ID (if special file)
off_t st_size;        // total size, in bytes
blksize_t st_blksize; // blocksize for filesystem I/O
blkcnt_t st_blocks;   // number of blocks allocated
time_t st_atime;      // time of last access
time_t st_mtime;      // time of last modification
time_t st_ctime;      // time of last status change
};
We can use the stat command-line tool to learn about a file.
stat filename으로 메타데이터 조회가능

Directory data is considered metadata, so you can only update a directory by creating files, directories, or other things within it. This is how the file system makes sure directory contents are correct.

The mkdir() call can create a directory from within an existing program. The mkdir program can create such a directory. The code below creates a basic directory called downloads with the mkdir program.

Instead of opening a directory as a file, the ls program uses three functions:
opendir()
readdir()
closedir()
We can see from the program to the left that ls is basically a loop that reads one directory entry at a time, printing out the name and inode number of each file in the directory.

struct dirent {
  char d_name[256];        // filename
  ino_t d_inode;             // inode number
  off_t d_offset;             // offset to the next dirent
  unsigned short d_recordlen; // length of this record
  unsigned char d_type;    // type of file
};

link() creates a new name in the directory and refers it to the same inode number as the original file.

Making a file completes two tasks by
Creating an inode structure that will track the file’s metadata, and
Linking that file to a human-readable name to be put in a directory.

The original file name and the new file name are both links to the underlying metadata about the file stored in inode number

When the file system unlinks a file, it checks the inode number’s reference count or link count. The reference count keeps track of how many files have been linked to this inode.

A symbolic link, or soft link is similar to creating a hard link, but a bit different. A symbolic link is a different type of file. Just like regular files and directories, symbolic links are a third type of file that the system recognizes.

 File systems frequently have a broader set of techniques for varying levels of sharing.
The first type is the UNIX permission bits.
 
Permissions are divided into three categories:
what the file’s owner can do,
what a group can do, and
what anyone (known as other) can do.

A process has to first ask the operating system for permission to access a file. A file descriptor is returned if permission is given with the intent to enable read or write access.
Each file descriptor relates to an entry in the open file table. A file’s current offset and other important information are tracked in this entry.
Processes can use lseek() to modify the current offset, allowing random access to different regions of the file.
A process must use fsync() or similar functions to update persistent data. However, doing so correctly while keeping high performance is difficult.
You can use hard links or symbolic links to have several human-readable names point to the same underlying file. Consider their strengths and disadvantages before using them. Remember that removing a file only unlinks it from the directory hierarchy.
Most file systems allow you to turn sharing on and off. Permissions bits give a basic method for these controls. Access control lists provide more precise control over who can access and change data.

crash sinario는
data block,inode,bitmap중 하나,둘만 써지고 나머지는 실패할 수 있다. 

Superblock: fsck checks the superblock by comparing the file system size to the allotted blocks. Finding corrupt superblocks is the goal. In this case, the system (or administrator) can use a backup superblock.
Free blocks: Then fsck traverses the inodes, indirect blocks, double indirect blocks, etc. to determine the file system’s current block allocation. It relies on inode information to produce correct allocation bitmaps, thus any discrepancy between bitmaps and inodes is addressed. Also, all inodes are checked for use in the inode bitmaps.
Inode state: Every inode is checked for damage. For example: fsck checks each inode’s type field (e.g., regular file, directory, symbolic link, etc.). Unresolved inode field issues are dealt with by fsck and the inode bitmap is refreshed.
Inode links: fsck counts the links in each inode. A folder reference (or link) counts the number of folders that contain a file reference (or link). Each file and directory in the file system has its own link count, which is validated by fsck. It is necessary to fix the inode count if the newly calculated count does not match. An allocated inode is moved to the lost+found directory when it is discovered.
Duplicates: fsck checks for duplicate block references in inodes. Delete an obvious defective inode. Cloning the pointed-to block would provide each inode its own copy.
Bad blocks: Faulty block pointers are detected while scanning the pointer list. An address larger than the partition size is considered “bad”. fsck simply removes the pointer from the inode or indirect block.
Directory checks: fsck doesn’t understand user files, but directories contain file system information. For each directory entry, fsck ensures that the inode referenced is allocated, and that no directory is linked to more than once in the hierarchy.

Scanning a huge disk volume to find all allocated blocks and read the directory tree may take minutes or hours. fsck performance became prohibitive as disk size and RAID adoption expanded (despite recent enhancements).

Write-ahead logging, or journaling

With write-ahead logging, before an update is made on the disk, a note is made somewhere else on the disk stating what is about to happen.
If a crash happens during a structure overwrite, instead of scanning the entire disk, you can just refer to the note you wrote and try again. Journaling adds work during updates to reduce recovery effort.

With Linux ext3, a well known journaling file system, the disk is separated into block groups, and each block group has an inode bitmap, data bitmap, inodes, and data blocks. The new key structure is the journal, which takes up little space in the partition or elsewhere.

Five blocks are used here. TxB contains the final addresses of blocks I[v2], B[v2], and Db (the pending file system update) and a transaction identifier (TID).

Then comes physical logging, three blocks of the update’s specific physical content. The last block (TxE) carries the transaction ID.

Once this transaction is safely placed on disk, we checkpoint the file system and overwrite the existing structures. To do this, we write I[v2], B[v2], and Db to disk. So our initial operation sequence is:
Journal write: Write the transaction to the log, including a transaction-begin block, all pending data and metadata modifications, and a transaction-end block.
Checkpoint: Update the pending information and data in the file system.

We start with TxB, I[v2], B[v2], Db, and TxE. After these writes, we checkpoint I[v2], B[v2], and Db to their ultimate destinations on disk.

It uses two-step transactional writing to avoid this. It first writes all blocks except TxE to the journal. The diary will then look like this (if we keep appending):
After that, the file system writes the TxE block, leaving the journal in this safe state:

A file system can use the journal to recover from a crash.
If the crash happens before the transaction is successfully logged, the pending update is skipped.

It is possible to recover updates if the file system crashes after the transaction commits but before the checkpoint.

The journal superblock stores enough information to identify which transactions have not yet been checkpointed. This allows for faster recovery and cyclic log usage.
We’ll add another step to our protocol:
Journal write: Write the contents of the transaction (TxB and the update) to the log.
Journal commit: Write the transaction commit block (TxE) to the log; wait for it to finish.
Checkpoint: Update the file system content to their final destinations.
Free: Update the journal superblock to label the transaction free.

Unlike metadata journaling, ordered journaling does not write user data to the notebook

For metadata-only journaling, the data blocks should be written first.
File systems like Linux ext3 write data blocks before metadata. The protocol is as follows:
Data write: At final location, write data; wait for completion (waiting is optional; details below).
Journal metadata write: Write the begin block and metadata to the log; wait for writes to complete.
Journal commit: Write the transaction commit block (including TxE) to the log; wait for the write to finish.
Checkpoint metadata: Write the metadata update to its final file system location.
Free: Later, mark the transaction free in journal superblock.

Soft Updates are another way to maintain metadata consistency. For example, it writes a pointed-to data block to disk before the inode that points to it, so the inode never points to garbage.

Copy-on-write (COW) is also utilized in several file systems, notably Sun’s ZFS. Not overwriting existing files or directories, but simply updating unused storage space. The COW file system updates the root structure with fresh structure references.

Backpointer-based consistency (BBC) is another method. Writing is not ordered. Every block in the system gets a back pointer. The file system can tell if a file is consistent by looking at the forward pointer, or address in the inode or direct block. If so, everything has reached disk safely, and the file is consistent; if not, an error is returned.

With an extended form of the transaction checksum and a few other techniques, the optimistic crash consistency strategy issues as many writes to disk as possible while detecting discrepancies.

We introduced the issue of crash consistency and a few of its possible solutions.
On newer computers, the earlier way of generating a file system checker may be too slow. So journaling is now widely used.
In short, journaling reduces recovery time from O(disk volume size) to O(log volume size), greatly speeding up post-crash recovery.
Journaling can take several forms; the most common is ordered metadata journaling, which minimizes traffic to the journal while maintaining sufficient consistency guarantees for both file system information and user data.
Solid user data guarantees are undoubtedly one of the most critical things to provide; however, recent research shows that this field is still evolving.

The log-structured file system was created due to the following observations:
System memory expansion
The performance gap between random and sequential I/O is large
File systems perform badly on several typical tasks
File systems are not aware of RAIDs

LFS, or Log-structured File System, buffers and writes all changes (including metadata) into an in-memory segment. This segment is written to disk in one continuous sequential transfer to an unused section of the disk.

LFS uses a technique called write buffering to keep track of updates in memory before writing them to disk. This guarantees efficient disk usage.

The huge batch of changes is called a segment. A segment, here, is a relatively large chunk utilized by LFS to aggregate writes.

When writing to disk, LFS buffers the updates in memory and writes the segment all at once.

어느정도의 데이터를 모아서 보내는게 효율적인지에 대한 연산. Rpeak과 Tposition은 고정값이므로 효율이 일정 이상이게 하기위한 D의 값을 구하는거다.

To find an inode in a normal UNIX file system or a system like FFS, inodes are structured in an array and placed in defined positions on disk.
This is more difficult in LFS because we’ve spread the inodes around the disk, and we never overwrite in place, so the desired inode keeps shifting.

LFS designers created a. level of indirection and a data structure called an inode map (imap). This structure takes an inode number as input and returns the most recent version’s disk address. It’s commonly implemented as a 4-byte array (a disk pointer). When an inode is written to disk, the imap is updated.

inode 맵은 파일 시스템의 구조 중 하나로, 리눅스 및 UNIX 계열 운영 시스템에서 파일의 메타데이터를 저장하는 데 사용됩니다. inode는 "index node"의 약자로, 파일 시스템 내 각 파일과 디렉토리에 대한 정보를 담고 있는 데이터 구조입니다. inode 맵은 이러한 inode들의 위치와 사용 여부를 추적하는 데 도움을 줍니다.

inode에는 파일의 소유자, 파일 권한, 파일 크기, 파일이 생성, 수정, 접근된 시간 등의 메타데이터와 파일 데이터가 저장되는 디스크 상의 위치 정보가 포함됩니다. 하지만 inode에는 파일 이름이 저장되지 않습니다. 파일 이름과 inode 번호는 디렉토리 엔트리에 저장되어, 파일 이름을 통해 해당 파일의 inode 정보에 접근할 수 있게 합니다.

inode 맵은 파일 시스템을 초기화할 때 생성되며, 각 inode에는 고유한 번호가 할당됩니다. 파일 시스템이 파일이나 디렉토리에 접근할 때, 해당 inode 번호를 사용하여 필요한 메타데이터와 파일 데이터의 위치를 빠르게 찾을 수 있습니다. 이 구조 덕분에 리눅스 및 유닉스 기반 시스템은 효율적인 파일 관리와 빠른 접근 속도를 제공할 수 있습니다.

inode 맵의 중요한 특징 중 하나는 고정된 크기를 갖는다는 것입니다. 이는 파일 시스템 내에 생성할 수 있는 파일과 디렉토리의 최대 개수를 제한합니다. 시스템이 많은 수의 작은 파일을 다룰 경우, inode 부족 현상이 발생할 수 있으며, 이는 파일 시스템의 확장이나 재구성을 필요로 할 수 있습니다.

inode 맵과 inode 구조는 리눅스와 유닉스 파일 시스템의 핵심 요소로, 효율적인 파일 관리와 시스템의 안정성을 보장하는 데 중요한 역할을 합니다.

Due to the inode map being fragmented over the disk, a file lookup requires a fixed and known location on disk.
This fixed location is known as the checkpoint region (CR) in LFS. The latest inode map parts are found by first reading the CR. The checkpoint region is only updated every 30
 seconds or so. A checkpoint region points to the newest portions of the imap. The imap contains addresses of inodes, which lead to files (and directories) like ordinary UNIX file systems.

메모리가 없는 상태에서 디스크에서 파일을 읽는 과정을 살펴보겠습니다. 체크포인트 영역은 먼저 읽혀지는 첫 번째 온디스크 데이터 구조로, 전체 inode 맵에 대한 포인터를 포함하고 있어, LFS(Log-structured File System)는 전체 imap(inode map)을 읽어 메모리에 캐시합니다.

파일의 inode 번호는 LFS에 의해 읽혀지며, LFS는 imap에서 inode 번호를 디스크 주소 매핑으로 조회하고 가장 최근 버전을 읽습니다.

파일에서 블록을 읽기 위해, LFS는 필요에 따라 직접 포인터, 간접 포인터, 또는 이중 간접 포인터를 사용합니다.

이 과정은 파일 시스템이 디스크에서 데이터를 효율적으로 검색하고 읽는 방법을 설명합니다. 체크포인트 영역에서 시작하여 필요한 데이터 구조와 파일의 실제 데이터에 접근하기까지의 단계를 포함합니다. 이러한 메커니즘은 파일 시스템의 성능과 안정성을 보장하는 데 중요한 역할을 합니다.


파일에 접근할 때, 일부 디렉토리도 열어야 합니다. 디렉토리 구조는 고대 UNIX 파일 시스템에서와 매우 유사하며, 이름과 inode 번호의 매핑을 포함합니다. 예를 들어, 파일을 생성할 때 LFS(Log-structured File System)는 새로운 inode, 일부 데이터, 파일의 디렉토리 데이터 및 inode를 작성해야 합니다. LFS는 일정 시간 동안 업데이트를 버퍼링 한 후 순차적으로 이를 수행합니다. 이는 디스크에 다음과 같은 새로운 구조를 결과로 낳습니다:

inode 맵은 디렉토리 파일 dir과 새로 생성된 파일 f에 대한 정보를 제공합니다. 이 예에서, 먼저 inode 맵에서 디렉토리 inode A3을 검색하고(일반적으로 RAM에 캐시됨), 디렉토리 inode를 읽어 디렉토리 데이터(A2)를 찾고, 이 데이터 블록을 읽어 파일 test의 이름-인덱스 번호 매핑(test, k)을 얻습니다. 그런 다음 inode 맵에서 inode k(A1)를 조회하고, 마지막으로 원하는 데이터 블록을 주소 A0에서 읽습니다.

inode 맵은 또한 LFS의 또 다른 중요한 문제인 재귀적 업데이트 문제를 해결합니다. 디스크의 새 위치로 업데이트를 전송하는 모든 파일 시스템(LFS와 같은)에서 이 문제가 존재합니다.

LFS는 inode 맵을 사용하여 파일에 해당하는 디렉토리를 업데이트하는 것을 피합니다. inode의 위치가 변경되더라도 디렉토리의 imap 구조는 변경되지 않습니다. LFS는 간접 참조를 통해 재귀적 업데이트 문제를 회피합니다.

LFS(Log-structured File System)와 관련된 문제 중 하나는 디스크 전체에 오래된 파일 구조 버전을 흩어지게 한다는 것입니다.

inode 번호 k로 참조된 파일을 고려해 보겠습니다. 이 파일은 단일 데이터 블록 D0에 연결됩니다. 새로운 inode와 데이터 블록이 생성될 때마다, 이전 블록을 업데이트합니다. (참고: 간단히 하기 위해, imap 및 기타 구조체는 생략하고, 새로운 inode를 참조하기 위해 디스크에 새로운 imap 조각을 작성해야 합니다.):

도표에서 볼 수 있듯이, inode와 데이터 블록에는 디스크 상에 두 버전이 있습니다. 하나는 오래된 것(왼쪽)이고 다른 하나는 현재 및 활성 상태인 것(오른쪽)입니다. 데이터 블록을 논리적으로 변경함으로써, LFS는 새로운 구조를 유지하고 오래된 버전을 디스크에 남겨둬야 합니다.

대신 원래 파일 k에 블록을 추가한다고 가정해 봅시다. inode는 다시 생성되지만, 여전히 원래 데이터 블록을 가리킵니다. 따라서 여전히 거기에 있고 현재 파일 시스템의 일부입니다:

LFS는 주기적으로 오래되고 사용되지 않는 파일 데이터, inode 및 기타 구조체의 오래된 버전을 찾아 청소하여 나중에 쓰기 위한 디스크 블록을 확보합니다. 청소는 가비지 컬렉션의 한 형태로, 컴퓨터 언어에서 메모리를 자동으로 확보하는 데 사용됩니다.

큰 쓰기를 디스크에 허용하는 메커니즘으로서 세그먼트는 성공적인 청소에 필수적입니다.

세그먼트를 사용하여 청소를 하면 시스템이 할당된 공간과 섞인 자유 공간의 구멍을 피할 수 있습니다.

록의 생존 여부를 결정하기 위해, LFS(Log-structured File System)는 각 세그먼트 블록에 약간의 추가 정보를 포함합니다. 이 정보에는 각 데이터 블록에 대한 inode 번호와 오프셋이 포함됩니다. 이 데이터는 세그먼트의 머리 부분에 위치한 세그먼트 요약 블록에 저장됩니다.

블록의 상태를 결정하는 것은 주소 A에서 블록 D의 inode 번호(N)와 오프셋(T)을 찾는 것으로 시작됩니다. 다음으로, imap에서 N을 찾아 디스크나 메모리에서 읽습니다. 마지막으로, 오프셋 T를 사용하여 디스크에서 파일의 T번째 블록을 찾습니다. LFS는 블록 D가 디스크 주소 A에 연결되어 있으면 블록 D가 생존해 있음을 결정할 수 있습니다. 다른 곳을 가리키면 LFS는 D가 더 이상 필요하지 않으며 이 버전이 죽었다는 것을 알 수 있습니다. 다음은 의사 코드 요약입니다:
(N, T) = SegmentSummary[A];
inode = Read(imap[N]);
if (inode[T] == A)
// 블록 D는 생존해 있음
else
// 블록 D는 쓰레기임

세그먼트 요약 블록(SS)은 주소 A0에 있는 데이터 블록이 실제로 파일 k의 오프셋 0의 일부분임을 기록합니다. k에 대한 imap을 확인함으로써 inode를 찾을 수 있습니다.

LFS는 생존 여부 결정 절차를 가속화하기 위해 여러 단축 방법을 사용합니다. 파일이 파괴될 때, LFS는 버전 번호를 증가시키고 imap를 업데이트합니다. 디스크 상의 버전 번호와 imap 버전 번호를 비교함으로써, LFS는 불필요한 읽기를 방지하고 위에 설명된 긴 검사를 우회할 수 있습니다. LFS는 세그먼트에 쓰기를 버퍼링한 다음, 세그먼트가 가득 차거나 일정 시간이 지나면 세그먼트를 디스크에 씁니다.

LFS는 이 쓰기들을 로그에 조직하며, 헤드와 꼬리 세그먼트를 가리키고 각 세그먼트가 다음에 쓸 세그먼트를 가리킵니다.

LFS가 세그먼트나 CR(Checkpoint Region)에 쓰는 동안 충돌이 발생할 수 있습니다.

세그먼트에 쓰기 중 충돌이 발생하면 데이터 손실을 최소화하기 위해 LFS는 디스크 양쪽 끝에 CR을 두 개 유지하고 번갈아 가며 업데이트를 수행하여 원자적인 CR 업데이트를 보장합니다.

LFS는 새로운 inode 맵 포인터와 기타 데이터를 CR에 업데이트하는 엄격한 프로토콜을 사용합니다; 헤더(타임스탬프 포함), 그 다음 본문, 그리고 마지막으로 하나의 블록(또한 타임스탬프 포함)을 내보냅니다.

LFS는 CR 업데이트 중 시스템 충돌을 타임스탬프의 일관성 없는 쌍을 통해 식별할 수 있습니다. LFS는 일관된 타임스탬프를 가진 가장 최근의 CR을 항상 사용할 것입니다.

LFS가 CR에 쓰는 동안 충돌이 발생하고 LFS가 다시 시작되면, LFS는 체크포인트 영역, 그것이 가리키는 imap 조각, 그리고 아직 읽지 않은 모든 파일이나 디렉토리를 읽을 것입니다.

LFS는 "롤 포워드"라고 불리는 데이터베이스 접근 방식을 사용하여 많은 세그먼트를 재구성하려고 시도합니다.

이 접근 방식은 마지막 체크포인트 영역을 사용하여 로그의 끝을 결정하고, 그것을 사용하여 다음 세그먼트를 검색하며 유효한 업데이트를 찾습니다. LFS는 마지막 체크포인트 이후에 작성된 많은 데이터와 메타데이터를 복구할 수 있습니다.

LFS(Log-structured File System)는 디스크 업데이트를 수정합니다. LFS는 항상 디스크의 사용되지 않은 부분에 쓰고 나중에 그것을 청소합니다. LFS는 모든 업데이트를 메모리 내 세그먼트로 그룹화하고 순차적으로 쓰는 방식으로 효율적으로 쓸 수 있습니다.

LFS의 대규모 쓰기는 많은 장치에서 빠릅니다. RAID-4와 RAID-5와 같은 패리티 기반 RAID에서는 대규모 쓰기가 작은 쓰기 문제를 제거합니다. 고성능을 위해 플래시 기반 SSD에서는 큰 I/O가 필요하므로, LFS 스타일의 파일 시스템이 적합할 수 있습니다.

이 절차는 폐기물(즉, 오래된 데이터 사본이 디스크 주변에 퍼져 있으며, 이러한 공간을 미래 사용을 위해 회수하려면 주기적으로 오래된 세그먼트를 청소해야 함)을 생산합니다. 청소 비용에 대한 우려는 LFS의 초기 영향을 제한했을 수 있습니다.

NetApp의 WAFL, Sun의 ZFS, Linux btrfs 및 심지어 더 새로운 플래시 기반 SSD와 같은 최근 상용 파일 시스템은 LFS의 개념적 유산을 이어받았습니다. 파일 시스템의 스냅샷을 제공함으로써, 사용자는 실수로 새 파일을 삭제한 경우 오래된 파일에 접근할 수 있습니다.

Flash, which we will explore here, has certain unique properties:
to write to a flash page, you must first erase a larger piece called a flash block, which might be costly and
writing on a page too often will wear it out.

Each transistor in a flash chip stores one or more binary values mapped to the level of charge trapped therein. There are different types of cells:
single-level cells (SLC): storing only a single bit (i.e., 1 or 0). These chips are faster and more expensive.
multi-level cells(MLC): storing two bits having differing levels of charge (e.g., 00, 01, 10, and 11 are representing low, somewhat low, somewhat high, and high levels respectively), and
triple-level cells (TLC): storing 3 bits per cell.
Flash chips are grouped into large banks or planes of cells.
In a bank, data is accessed in two sizes: blocks (sometimes called erase blocks) of 128 KB or more, and pages of a few KB (e.g., 4KB).

Each bank has several blocks, and each block has many pages. With flash, blocks and pages are distinct from disk blocks, RAID blocks, and virtual memory pages.

With this flash arrangement, a flash chip can handle three low-level processes:
Read a page: A flash chip client can read any page by sending the read instruction and the appropriate page number.
This action is often very quick, tens of microseconds or less, regardless of device location or previous request location. This being a random access device it can swiftly access any place.
Erase a block: Before writing to a page in a flash, you must first erase the block it is contained in.
To avoid losing data, you must ensure that it has been moved to another location before issuing the erase command. The wipe instruction takes a few milliseconds to complete. The block is then reset and each page is ready to program.
Program a page: After erasing a block, the program command can be used to convert some 1s to 0s and write the desired page contents to the flash.
Programming a page is faster than deleting a block, but slower than reading a page on current flash devices.

Each page in the flash chips has an associated state. Pages begin as INVALID.
All pages within a block are set to ERASED when its block is erased. This resets their content and makes them programmable.
A page’s status turns to VALID once it is programmed. Programmed means that a page’s contents are set and may be read. Reads are not affected by these states.
Once a page is programmed, the only way to modify its content is to remove the entire block.

The standard storage interface reads and writes   512
 -byte blocks (or larger). Over the raw flash chips, the flash-based SSD provides a conventional block interface.
SRAM (static random access memory) is used for data caching, buffering, and mapping tables. It also has device control logic.

Using multiple flash chips in parallel and write-amplification (FTL’s total write traffic divided by the client’s total write traffic) both increase performance.
To improve reliability, the FTL tries to evenly distribute writes across the flash blocks. As a result, all blocks will wear out at the same time. This is called wear leveling and is standard in current FTL.

A direct mapping FTL A logical page N
 read corresponds to a physical page N
 read. It must first read the entire block containing page N
, then erase it, and then program the old and new pages.
Direct-mapped FTL suffers from performance and reliability difficulties. Performance concerns arise from having to read in, erase, and program the entire block for each write. There is a lot of write amplification (proportional to the block size) and poor write performance.
Also untrustworthy. Reprogramming the same block quickly wears it out and loses data. Insufficient write load distribution among logical blocks leads to premature wear out of the underlying physical blocks containing popular data. It is inconsistent and slow.


Most FTLs use logs. A write to logical block   N
  adds   N
  to the current block’s free space. This is called logging. The device maintains a mapping table including the physical addresses of all logical blocks.

This basic log structuring method has several flaws. The first is that overwriting logical blocks generates garbage, which requires frequent collection. Second, in-memory mapping tables are expensive. Larger devices require more memory.

The second cost of log-structure is one item every 4
-KB device page. A single 4
-byte item on a 4
-KB page takes up 1
 GB of memory on a 1
-TB SSD. Page-level FTL is unusable.
This reduces mapping costs by Sizeblock
 divided by Sizepage
. block-level Greater virtual memory page sizes require less VPN bits and a wider offset in each virtual address.
A log-based FTL with block-based mapping is sluggish. The fundamental issue is when a “small write” is smaller than a physical block. In this situation, the FTL must read and duplicate the old block’s live data (along with the data from the small write).

Using block-level mapping cuts down on the number of address translations that the FTL has to do.

modern FTLs use hybrid mapping. In hybrid mapping, the FTL keeps a few erased blocks and directs all writes to them. The FTL does not wish to copy these log blocks, thus it saves per-page mappings for these log blocks.

FTL keeps two types of tables:
the log-table which stores a small set of per-page mappings
the data table which stores the per-block mappings.

If the FTL can’t find a logical block in the log-table, it looks in the data table to find it.

Flash-based SSDs are becoming prevalent in laptops, desktops, and servers in the world’s data centers.
A flash chip has multiple banks, each structured into erase blocks (sometimes just called blocks). Each block is broken into pages.
Blocks are large (128KB
–2MB
) and contain several smaller pages (1KB
–8KB
).
To read from flash, send a read command with an address and length.
Flash is more difficult. Initially, the client must erase the entire block (which deletes all information within it). Once programmed, the client can complete the write.
A new trim operation is important for informing the device that a certain block (or set of blocks) is no longer required.
Reliability of flash is largely controlled by wear out; too much erasing and programming destroys flash.
A flash-based solid-state data works like a regular block-based read/write disk, by using a flash translation layer, it transforms client reads and writes into reads, erases, and programs to underlying flash chips.
Most FTLs are log-structured, reducing writing costs by reducing erase/program cycles. An in-memory translation layer tracks physical writes.
The cost of garbage collection leads to write amplification in log-structured FTLs.
Another issue is the mapping table’s size, which can be rather extensive. Remedies include hybrid mapping or only caching hot FTL parts.
Last but not least, wear leveling requires the FTL to migrate data from blocks that are mostly read to guarantee they get their fair portion of erase/program load.


distrubted systems
Connecting to a web server anywhere in the globe seems straightforward, but it isn’t. These complex services are made up of many machines that work together to make the site work.

We don’t know how to develop “perfect” parts or systems, so we focus on how to make a modern online service seem like it never fails to its users.
The goal is to build systems that work even when parts of them don’t.
Failure is a big problem when it comes to building distributed systems.
Even if one machine fails, that doesn’t mean the entire system must fail. This is one of the great things about distributed systems, and it’s why Google, Facebook, and other web services use them now.

Another problem is performance.
Because our distributed system is connected by a network, system designers have to think about ways to keep the number of messages sent down and make communication as efficient as possible.

A final concern is security.
A connection to a remote site must have some trust that the person on the other side is who they say they are and trust that third party cannot monitor or change an ongoing communication between two others.

Communication is new in distributed systems. Specifically, how machines in a distributed system talk to each other.
In this section, we’ll start with the simplest things first, like messages, and work our way up from there.

It appears that communication is always going to be unreliable with modern networking. Packets are frequently lost, corrupted, or otherwise not delivered over the Internet or through high-speed local networks.

To communicate with UDP, a process uses the sockets API to build a communication endpoint. Processes on the same or a different machine send UDP datagrams which are a fixed-sized message up to some max size.

TCP에 대한 설명

The systems community came up with many approaches to communication abstraction.
One approach, distributed shared memory (DSM)systems, made it possible for processes on different computers to share a huge virtual address space. This allowed distributed computation to run multiple 

Almost all of the DSM systems work with the OS’s “virtual memory” in one of two ways:
The page is already local on the machine (best case) because the data can be quickly retrieved, or
The page must be retrieved from another machine to move on. In this case, the page fault handler contacts the other system to get the page and put it in the process’s page table.
DSM’s biggest flaws are:
How it deals with failure -
If one machine fails, then parts of the data structure or computations become instantly unavailable.
Its performance -
Some DSM accesses are cheap, but others are very expensive.
DSM programmers had to be very careful about how computations were done so that very little communication took place, which was against the point of the approach.threads on different machines instead of on different processors on the same machine.

Despite a lot of research, DSM is no longer used to make sure that distributed systems are safe.

Programming language abstractions make far more sense than OS abstractions for constructing distributed systems. The most common abstraction is a remote procedure call, or RPC.
The purpose of all remote procedure call packages is to make executing code on a distant machine as straightforward as calling a local function.
A procedure call is made to a client, and the results are returned after some time. The server merely specifies which routines to export.
The RPC system handles the rest with a stub generator (sometimes called a protocol compiler) and a run-time library.

Code is also generated for the server:
Unpack the message.
Unmarshaling or deserialization removes the information from the incoming message.
Arguments and function identifiers are extracted from the message.
Call into the function.
We have reached the point where the remote function is executed. Runtime RPC calls the function specified by the ID and passes the arguments.
Package the results.
The return argument(s) are marshaled back into a single reply buffer.
Send the reply to the caller.
A stub compiler must also consider a few more factors. To begin, how does one package and convey a complex data structure?
Say, an integer file descriptor, a buffer pointer, and a size argument are passed to the write() system call.
If an RPC package receives a pointer, it must interpret it and take the appropriate action.
Usually, this is done by annotating the data structures with more information, allowing the compiler to know which bytes need to be serialized.
Another important thing to think about is how the server handles concurrent tasks.
One by one, requests come into a simple server and it waits for them in a loop.
Because if one RPC call takes too long, this can get wildly inefficient.
Most servers are often built concurrently using thread pools.
When the server starts, a limited number of worker threads are created.
When a message comes in, it is sent to one of these worker threads, which then performs the RPC call and sends back the answer.
While this is going on, the main thread is still receiving messages and may send them to other workers.
The standard costs also come up, but they mostly come in the form of more complicated programming. The RPC calls may now need locks and other synchronization tools to make sure they work properly.

The run-time library handles the bulk of the work and solves most performance and reliability issues in an RPC system.
One issue we do see frequently in distributed systems is naming.
The simplest approaches rely on existing naming systems, such as hostnames and port numbers (a way of identifying a particular communication activity taking place on a machine).
This way, once the client knows the RPC server’s hostname or IP address and the port number it uses, the internet protocols then allow any machine in the system to send packets to a certain address.
Once a client knows which server to contact for a certain service, the question becomes which transport-level protocol to use.
Putting RPC on top of a reliable communication layer could lead to performance issues.
Remember how reliable communication layers work: acknowledgments plus timeout/retry.
When a client delivers an RPC request to a server, the server acknowledges the request to let the caller know it was received.
Similarly, when the server replies to the client, the client acknowledges receipt. Building an RPC protocol on top of a reliable communication layer would send two “extra” messages.
As a result, many RPC packages rely on unreliable communication protocols like UDP. This allows for a more efficient RPC layer, but also adds the duty of providing reliability.
Using timeout/retry and acknowledgments, the RPC layer achieves the necessary amount of responsibility.
The communication layer can ensure that each RPC occurs exactly once or, at most, once by employing sequence numbering.

We have seen the introduction of distributed systems and its fundamental issue: handling failure. Failure is unusual on a desktop machine, but common in a data center with thousands of machines. Managing failure is critical in distributed systems.
A distributed system’s heart is communication. The RPC package handles all the details, including timeout/retry and acknowledgment, to produce a service that closely resembles a local procedure call.
The best approach to learn an RPC package is to use it.

Client applications use the client-side file system to access files and directories. To access files stored on the server, a client program uses the client-side file system to make system calls (open(), read(), write(), close(), mkdir(), etc.). This makes the file system appear the same as the client’s local (disk-based) file system.
In this approach, distributed file systems enable transparent access to files.
The client-side file system handles system calls. Examples include when a client calls read(). The client-side file system instructs the server-side file system to read from a specific disk block (or its own in-memory cache).
It will then copy data into the user buffer supplied to read(), finishing your query. In this case, the block may be cached in memory or disk. No network traffic is generated.

These are the two key components to a client/server distributed file system:
the client-side file system, and
the file server.
Their behavior together determines how the distributed file system functions.

Instead of creating a proprietary and closed system, Sun created an open protocol that merely defined the message formats that clients and servers would use to interact.
Different parties could design their own NFS servers and compete as long as they preserve interoperability.

This widespread success is likely a testament to NFS’s open-market approach.

The classic NFS protocol (sometimes known as NFSv2) was the standard for many years.
The protocol design goal for NFSv2 was simple with rapid server recovery.
This approach makes a lot of sense if you are dealing with a multiple-client, single-server environment. If the server fails, the entire system fails.

NFSv2 achieved the goal of fast crash recovery by designing a stateless protocol.
To begin, the server does not track client activity; rather, the protocol is designed to send all relevant information in each protocol request.

Lets examine a stateful protocol. Consider the open() system call. If you give it a pathname, open() returns an integer file descriptor. In the code to the left, the descriptor is used to read() or write() to different parts of a file.

Think about how a file system on the client-side file system opens the file.
The client sends the server a protocol message that says, “Open the file notes and return a descriptor.”
The file server opens the file and sends its descriptor to the client.
It then calls read() with the descriptor, and the client-side file system sends the descriptor along with the request for a file.

Here, the file descriptor is a shared state (or distributed state) between the client and server.

Shared state, as stated above, impedes(방해하다) recovery. Assume the server crashes after the first read but before the client sends the second.

After the server restarts, the client issues the second read, but the information was lost in the crash.
To address this, the client and server would need to have a recovery protocol, where the client would keep enough information in its memory to inform the server of the situation (the file descriptor fd refers to file notes).

A stateful server must also handle client crashes. Assume a user opens a file and crashes. open() uses a file descriptor, so the server doesn’t know when to close it. Normally, a client would call close() to close a file. When a client dies, the server never gets a close().

In order to address these issues, NFS designers chose a stateless approach:
Each client operation contains all information needed to complete the request.

The file handle is important in understanding the NFS protocol design.
Many protocol requests include a file handle to uniquely identify the file or directory being operated on.
A file handle has three key components:
Volume identification
tells the server which file system the request is for. An NFS server can export more than one file at a time, so this is important.
Inode number
tells the server which file in that partition the request is going to.
The client’s generation number
when reusing an inode number If an old file handle is used, the generation number is incremented, preventing inadvertent access to the freshly allocated file.

The file to the left holds a summary of the major pieces of the protocol.
To access files, the LOOKUP protocol message first obtains a file handle.
The client sends the server a directory file handle and a file name to lookup, and the server returns the handle and related attributes, such as the file’s creation and modification dates, size, ownership and permissions. Similar to what you’d get from a file’s stat() call.

Imagine the client already has a handle to the file system’s root directory (/) which it would have obtained through the NFS mount protocol.

If a client-side program opens the file /essay.txt, it would send a lookup request for the file handle (and attributes) for the file essay.txt.
Now, the client can use the READ and WRITE protocol messages to read and write files.
The READ protocol message requires the protocol to send:
the file handle,
offset within the file,
the quantity of bytes to read, and
the file name.

The server can then read the data from the file (since the handle tells it which volume and inode to read from) and provide it to the client (or an error if there was a failure).

Writing data to the server is done similarly, except that no data is sent back.

One additional protocol to mention is the GETATTR request. The GETATTR will collect the file’s attributes like the file’s last update time, given only the file handle.

The protocol used between the client and a file servers should be getting a little clearer. The client-side file system keeps track of open files and turns application requests into relevant protocol messages. The server will then respond to protocol messages, which will have all the information needed.

Let’s say we have a small application that reads a file.

First, note how the client keeps track of the relevant state including the file descriptor’s NFS file handle mapping as well as the current file pointer. This allows the client to convert each read request into a properly designed read protocol message telling the server exactly which bytes from the file to read. Following a successful read, the same file handle is used, but the offset is different for each one.

Second, we note where the interactions with the server take place. If the pathname is long (like, /home/codio/workspace/essay.txt), the client would require four lookups:
one for home,
one for codio in home,
one for workspace in codio, and
one for essay.txt in workspace.
Third, notice how each server request has all that it needs to complete the request.

server-client간 전송,실패 시나리오시

NFS queries are idempotent(멱등성(연산을 여러 번 적용하더라도 결과가 달라지지 않는 성질)), the client can simply retry. 
An operation is idempotent if the result of repeating it is the same. 
Idempotent actions include, for example, storing a value three times in memory. Notably, incrementing a counter three times yields a different outcome than incrementing it once. A read-only operation is usually idempotent, but an update operation must be carefully analyzed.

The idempotency of most common operations lies at the basis of NFS crash recovery. In the absence of an update, LOOKUP and READ queries are trivially idempotent. WRITE requests are also idempotent. If a WRITE fails, the client can simply try again. Data, count, and (most critically) offset are all contained in the WRITE message. It can be repeated knowing that the consequence of several writes is the same as a single write.

some operations are difficult to idempotent. For example, when trying to create a directory that already exists, the mkdir request fails. The file server receives an MKDIR protocol message and executes it properly, but the reply is lost. The client may repeat the action and face that failure.

Sending all read and write requests over the network poses a huge performance issue: the network is slower than local memory or disk.
How can we measure a distributed file system’s performance?
Client-side caching, as the huge bold words in the sub-heading above suggest. NFS caches file data (and metadata) read from the server in client memory. While the first access is costly and requires network connectivity, subsequent accesses served from client memory are inexpensive.
The cache also acts as a write buffer. Before writing to a file, a client application buffers the data in client memory. The application’s write() function succeeds instantly and only puts the data in the client-side file system’s cache. The data is only written to the file server afterwards.
NFS clients cache data and perform well, right? Well, no. A system with many client caches creates a huge and interesting challenge we’ll call the cache consistency problem.

NFSv2 addresses cache consistency in two ways. By writing to and closing a file, a client software flushes all updates (dirty cache pages) to the server. With flush-on-close, another node will see the latest file version.
NFSv2 clients also check for changes in a file before accessing its cached data. Each cached block generates a GETATTR request to the server. The client invalidates the file if the server time-of-modification is more recent than the client cache time-of-modification. This clears the client cache and guarantees that subsequent reads get the latest version. The client will use the cached data if it has the latest version of the file.

The stale cache method resulted in an overflow of GETATTR requests on the NFS server. Even if only one client accessed a file (perhaps frequently), the client had to make GETATTR queries to the server to ensure no one else had changed it. That’s why a client keeps asking the server “has anyone edited this file?”
Now every client has an attribute cache. In the attribute cache, clients still examine files before accessing them. A file’s attributes were cached and timed out when accessed (say 3
 seconds). If the cached file was safe to use, all file accesses would proceed without contacting the server.

Final thoughts on NFS cache consistency. While the flush-on-close behavior made sense, it caused a performance issue.
The server would still be required to accept a temporary or short-lived file created on a client and subsequently erased. A better method would hold such temporary files in memory until they are erased, reducing server contact and possibly improving performance.
Adding an attribute cache to NFS made it difficult to determine what version of a file one was getting. Because your attribute cache had not yet timed out, the client was delighted to offer you whatever was in client memory. This was great most of the time, but occasionally led to weird behavior.

NFS servers must commit each write to persistent storage before telling the client of success.
The issue with this requirement in NFS server implementation is that write performance might be a substantial bottleneck.
Companies like Network Appliance were created with the goal of building an NFS server that performs writes quickly. They do this using a few work-arounds:
For example, they put writes in a battery-backed memory, allowing rapid answers to WRITE requests without danger of losing data or incurring the cost of writing to disk immediately.

Initiation of NFS (Network File System NFS is meant to allow simple and rapid server recovery. Because a client can safely replay a failed action, it doesn’t matter if the server did it.
Caching can potentially complicate a multi-client, single-server system. It’s reasonable to address cache consistency, which NFS does carelessly, resulting in weird behavior. To sum up, forcing writes to stable storage before returning success (otherwise data can be lost).
We haven’t discussed other aspects, most notably security. Early NFS security was inadequate; any user on a client could easily masquerade as another user and so access nearly any file.
An NFS stateless protocol is the key to achieving the core goal of rapid and simple crash recovery. After a server crash, clients just retry requests until they succeed.
Making requests idempotent is a central aspect of the NFS protocol. An operation is idempotent if repeating it has the same result as doing it once. In NFS, idempotency unifies client lost-message retransmission and client-server crash handling.
Client-side caching and write buffering are required for performance, but pose a cache consistency issue.
NFS implementations engineer cache integrity in different ways: When a file is closed, its contents are forced to the server, allowing other clients to see the updates. An attribute cache decreases the frequency of file change checks with the server (via GETATTR requests).